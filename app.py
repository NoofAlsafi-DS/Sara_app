import os
import re
import urllib.parse
import pickle
import numpy as np
import streamlit as st
from sklearn.base import BaseEstimator, TransformerMixin
from scipy.sparse import hstack

# ==============================
# 1) ManualFeatures class (must match the name used when saving manual.pkl)
# ==============================
class ManualFeatures(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        feats = []
        for url in X:
            u = str(url or "")
            p = urllib.parse.urlparse(u)
            host = p.netloc or ""
            path = p.path or ""
            feats.append([
                len(u),                                   # total length
                u.count('-'),                             # '-' count
                u.count('@'),                             # '@' count
                u.count('?'),                             # '?' count
                u.count('%'),                             # '%' count
                u.count('.'),                             # '.' count
                sum(c.isdigit() for c in u),              # digits count
                1 if re.search(r'\b\d+\.\d+\.\d+\.\d+\b', host) else 0,  # IP in domain
                1 if u.lower().startswith('https') else 0,               # HTTPS flag
                len(path),                                # path length
            ])
        return np.array(feats, dtype=float)

# ==============================
# 2) Cached loaders for artifacts
# ==============================
# âš ï¸ ØªØ¹Ø±ÙŠÙ Ø§Ù„ÙƒÙ„Ø§Ø³ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¨Ù†ÙØ³ Ø§Ù„Ø§Ø³Ù…
class URLFeatureExtractor:
    def __init__(self):
        pass

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        import numpy as np
        import re, urllib.parse
        feats = []
        for url in X:
            u = str(url or "")
            p = urllib.parse.urlparse(u)
            host = p.netloc or ""
            path = p.path or ""
            feats.append([
                len(u),                                   # Ø·ÙˆÙ„ Ø§Ù„Ø±Ø§Ø¨Ø·
                u.count('-'),                             # Ø¹Ø¯Ø¯ '-'
                u.count('@'),                             # Ø¹Ø¯Ø¯ '@'
                u.count('?'),                             # Ø¹Ø¯Ø¯ '?'
                u.count('%'),                             # Ø¹Ø¯Ø¯ '%'
                u.count('.'),                             # Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø·
                sum(c.isdigit() for c in u),              # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
                1 if re.search(r'\d+\.\d+\.\d+\.\d+', host) else 0,  # ÙˆØ¬ÙˆØ¯ IP
                1 if u.lower().startswith('https') else 0,           # HTTPS
                len(path),                                # Ø·ÙˆÙ„ Ø§Ù„Ù…Ø³Ø§Ø±
            ])
        return np.array(feats, dtype=float)

@st.cache_resource(show_spinner=False)
def load_artifacts():
    try:
        with open("model.pkl","rb") as f:
            clf = pickle.load(f)
        with open("tfidf.pkl","rb") as f:
            tfidf = pickle.load(f)
        with open("manual.pkl","rb") as f:
            man = pickle.load(f)
        return clf, tfidf, man
    except Exception as e:
        st.exception(e)
        st.stop()

# Helper: safe probability extraction
def get_positive_prob(model, X):
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X)
        if proba is not None and proba.ndim == 2 and proba.shape[1] >= 2:
            return float(proba[0, 1])
    if hasattr(model, "decision_function"):
        val = model.decision_function(X)
        if np.ndim(val) == 1:
            return float(1.0 / (1.0 + np.exp(-val[0])))
    return None

# Helper: small feature preview for a single URL
def preview_manual_features(url: str):
    u = str(url or "")
    p = urllib.parse.urlparse(u)
    host = p.netloc or ""
    path = p.path or ""
    return {
        "length": len(u),
        "dash_count": u.count('-'),
        "at_count": u.count('@'),
        "question_mark": u.count('?'),
        "percent_count": u.count('%'),
        "dot_count": u.count('.'),
        "digits_count": sum(c.isdigit() for c in u),
        "has_ip_in_domain": bool(re.search(r'\b\d+\.\d+\.\d+\.\d+\b', host)),
        "is_https": u.lower().startswith('https'),
        "path_length": len(path),
    }

# ==============================
# 3) UI
# ==============================
st.set_page_config(page_title="ğŸ”’ URL Malware Detector", page_icon="ğŸ›¡ï¸", layout="centered")
st.title("ğŸ”’ URL Malware Detector")
st.caption("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¶Ø§Ø±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF + Ù…ÙŠØ²Ø§Øª ÙŠØ¯ÙˆÙŠØ© + Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ù‘Ø¨.")

with st.sidebar:
    st.markdown("## Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    st.markdown("Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª: `model.pkl`, `tfidf.pkl`, `manual.pkl`.")
    st.markdown("Ø¶Ø¹ ØµÙˆØ±ØªÙŠ `safe.png` Ùˆ `malicious.png` Ø§Ø®ØªÙŠØ§Ø±ÙŠÙ‹Ø§ Ù„Ø¹Ø±Ø¶ ØµÙˆØ±Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ù„Ù†ØªÙŠØ¬Ø©.")
    st.divider()
    st.markdown("### Ø£Ù…Ø«Ù„Ø© Ø¬Ø§Ù‡Ø²Ø©")
    examples_safe = [
        "https://www.wikipedia.org/",
        "https://www.openai.com/research/",
    ]
    examples_bad = [
        "http://198.51.100.23/login/verify?acc=123",
        "http://paypal.com.security-alert.example.com/confirm%20info",
    ]
    ex_col1, ex_col2 = st.columns(2)
    with ex_col1:
        if st.button("Ù…Ø«Ø§Ù„ Ø³Ù„ÙŠÙ…", use_container_width=True):
            st.session_state['sample_url'] = examples_safe[0]
    with ex_col2:
        if st.button("Ù…Ø«Ø§Ù„ Ø¶Ø§Ø±", use_container_width=True):
            st.session_state['sample_url'] = examples_bad[0]

default_text = st.session_state.get('sample_url', '')
url = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ù†Ø§:", value=default_text, placeholder="https://example.com/path?...")

analyze = st.button("ØªØ­Ù„ÙŠÙ„ ğŸ”", type="primary")

if analyze and url.strip():
    clf, tfidf, man = load_artifacts()

    try:
        X_tfidf = tfidf.transform([url])
    except Exception as e:
        st.error("ØªØ¹Ø°Ø± ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF. ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ù†Ø³Ø®Ø© scikit-learn/Ø§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ²Ø±.")
        st.exception(e)
        st.stop()

    try:
        X_manual = man.transform([url]) if hasattr(man, "transform") else ManualFeatures().transform([url])
    except Exception:
        X_manual = ManualFeatures().transform([url])

    try:
        X = hstack([X_tfidf, X_manual])
    except Exception as e:
        st.error("ØªØ¹Ø°Ø± Ø¯Ù…Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª ÙˆØ­Ø¬Ù…Ù‡Ø§.")
        st.exception(e)
        st.stop()

    try:
        pred = int(clf.predict(X)[0])
    except Exception as e:
        st.error("ØªØ¹Ø°Ø± Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤. ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª.")
        st.exception(e)
        st.stop()

    prob = get_positive_prob(clf, X)

    st.divider()
    left, right = st.columns([1,1])

    with left:
        if pred == 1:
            st.subheader("Ø§Ù„Ù†ØªÙŠØ¬Ø©: **Ø¶Ø§Ø±** âŒ")
            if os.path.exists("malicious.png"):
                st.image("malicious.png", caption="ØªØ­Ø°ÙŠØ±: Ø±Ø§Ø¨Ø· Ø¶Ø§Ø±", use_container_width=True)
            else:
                st.warning("ØµÙˆØ±Ø© 'malicious.png' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø£Ø¶ÙÙ‡Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø±Ø¦ÙŠ.")
        else:
            st.subheader("Ø§Ù„Ù†ØªÙŠØ¬Ø©: **Ø³Ù„ÙŠÙ…** âœ…")
            if os.path.exists("safe.png"):
                st.image("safe.png", caption="Ø±Ø§Ø¨Ø· Ø³Ù„ÙŠÙ…", use_container_width=True)
            else:
                st.info("ØµÙˆØ±Ø© 'safe.png' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø£Ø¶ÙÙ‡Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø±Ø¦ÙŠ.")

        if prob is not None:
            st.metric(label="Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© (Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù„Ø¶Ø§Ø±)", value=f"{prob:.3f}")
        else:
            st.caption("Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ ÙŠØ¯Ø¹Ù… `predict_proba`ØŒ Ø¹ÙØ±Ø¶Øª Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø¯ÙˆÙ† Ø¯Ø±Ø¬Ø© Ø«Ù‚Ø©.")

    with right:
        st.markdown("#### Ù…Ø¹Ø§ÙŠÙ†Ø© Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙŠØ¯ÙˆÙŠØ©")
        feats = preview_manual_features(url)
        st.dataframe({ "Ø§Ù„Ù…ÙŠØ²Ø©": list(feats.keys()), "Ø§Ù„Ù‚ÙŠÙ…Ø©": list(feats.values()) })

    with st.expander("ØªÙØ§ØµÙŠÙ„ ØªÙ‚Ù†ÙŠØ©"):
        st.write("â€¢ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙŠØ²Ø§Øª TF-IDF Ù„Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø±Ø§Ø¨Ø· + Ù…ÙŠØ²Ø§Øª ÙŠØ¯ÙˆÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø·ÙˆÙ„ ÙˆÙˆØ¬ÙˆØ¯ IP.\n"
                 "â€¢ Ø¥Ø°Ø§ ØªØºÙŠÙ‘Ø±Øª Ù†Ø³Ø® Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨ÙŠÙ† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ´ØºÙŠÙ„ Ù‚Ø¯ ØªØ¸Ù‡Ø± Ø£Ø®Ø·Ø§Ø¡ ØªÙˆØ§ÙÙ‚. Ø«Ø¨Ù‘Øª Ù†ÙØ³ Ø§Ù„Ù†Ø³Ø® ÙÙŠ requirements.txt.")

st.markdown("---")
st.caption("Â© 2025 â€” ØªØ·Ø¨ÙŠÙ‚ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ø§ ÙŠÙØ¹Ø¯ Ø£Ø¯Ø§Ø© Ø£Ù…Ù†ÙŠØ© Ù†Ù‡Ø§Ø¦ÙŠØ©. Ø§Ø³ØªØ®Ø¯Ù…Ù‡ ÙƒØ¥Ø±Ø´Ø§Ø¯ Ø£ÙˆÙ„ÙŠ ÙÙ‚Ø·.")
